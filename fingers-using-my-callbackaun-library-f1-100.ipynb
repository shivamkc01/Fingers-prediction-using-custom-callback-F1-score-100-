{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description\n## Context\nThe goal of the project is to build a model able to count fingers as well as distinguish between left and right hand.\n\n## Content\n21600 images of left and right hands fingers.\n\nAll images are 128 by 128 pixels.\n\n- Training set: 18000 images\n- Test set: 3600 images\n- Images are centered by the center of mass\n- Noise pattern on the background\n\n## Labels\nLabels are in 2 last characters of a file name. L/R indicates left/right hand; 0,1,2,3,4,5 indicates number of fingers.\n\n## Note\nImages of a left hand were generated by flipping images of right hand.","metadata":{}},{"cell_type":"markdown","source":"# Import need modules","metadata":{}},{"cell_type":"code","source":"!pip install callbackaun","metadata":{"execution":{"iopub.status.busy":"2022-03-20T20:42:28.718629Z","iopub.execute_input":"2022-03-20T20:42:28.719292Z","iopub.status.idle":"2022-03-20T20:42:36.111298Z","shell.execute_reply.started":"2022-03-20T20:42:28.719257Z","shell.execute_reply":"2022-03-20T20:42:36.110465Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport shutil\nimport time\nimport cv2 as cv2\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport callbackaun\nimport datetime\nimport logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib.pyplot import imshow\nfrom datetime import datetime\nfrom PIL import Image\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom callbackaun.alrcallback import ALRCallback\nfrom tensorflow.python.keras import regularizers\nsns.set_style('darkgrid')\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\npd.set_option('display.width', 2000)\ntf.autograph.set_verbosity(0)\nprint('modules loaded')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-20T22:04:00.821217Z","iopub.execute_input":"2022-03-20T22:04:00.821484Z","iopub.status.idle":"2022-03-20T22:04:00.833307Z","shell.execute_reply.started":"2022-03-20T22:04:00.821455Z","shell.execute_reply":"2022-03-20T22:04:00.832535Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"INPUT AN IMAGE AND GET THE SHAPE","metadata":{}},{"cell_type":"code","source":"img_path = r'../input/fingers/train/7194c4e9-8e19-496a-9c56-56678d40b67b_4R.png'\nimg = plt.imread(img_path)\nprint('Input image shape is', img.shape)\nplt.axis('off')\nimshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T20:42:36.127430Z","iopub.execute_input":"2022-03-20T20:42:36.127730Z","iopub.status.idle":"2022-03-20T20:42:36.338094Z","shell.execute_reply.started":"2022-03-20T20:42:36.127695Z","shell.execute_reply":"2022-03-20T20:42:36.337430Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"img_path = r'../input/fingers/test/ef512789-23fb-4da6-ad2e-38780d9cd914_1L.png'\nimg = plt.imread(img_path)\nprint('Input image shape is', img.shape)\nplt.axis('off')\nimshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T20:42:36.339860Z","iopub.execute_input":"2022-03-20T20:42:36.340649Z","iopub.status.idle":"2022-03-20T20:42:36.539371Z","shell.execute_reply.started":"2022-03-20T20:42:36.340596Z","shell.execute_reply":"2022-03-20T20:42:36.538662Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"Let's Display some training images","metadata":{}},{"cell_type":"markdown","source":"# Defining a function to print text in RGB foreground and background colors","metadata":{}},{"cell_type":"code","source":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:26:43.030185Z","iopub.execute_input":"2022-03-20T23:26:43.030443Z","iopub.status.idle":"2022-03-20T23:26:43.036003Z","shell.execute_reply.started":"2022-03-20T23:26:43.030416Z","shell.execute_reply":"2022-03-20T23:26:43.035302Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"\nIn Dataset Discription they mention that L/R indicate left/right hand\n\n#Labels\nLabels are in 2 last characters of a file name. L/R indicates left/right hand; 0,1,2,3,4,5 indicates number of fingers.","metadata":{}},{"cell_type":"code","source":"def preprocess(src_dir, train_split):\n    categories = ['train', 'test']\n    for category in categories:\n        cat_path = os.path.join(src_dir, category)  # addting categories to src_dir like adding \"/train\" or \"/test\" in your directory\n        \n        file_paths = []\n        fingers = []\n        hands = []\n        both = []\n        file_list = os.listdir(cat_path)  # listing all data within train folder and test folder\n        # for example file_list = ['train_data1.png', 'test_data1.png']\n        for f in file_list:\n            \n    \n            file_path = os.path.join(cat_path, f) # joining train folder with image name like ../train/train_data1.png\n            file_paths.append(file_path)   # adding file path to file path list\n            index = f.rfind('.')  # rfind() method returns the highest index of the substring if found in the given string. If not found then it returns -1.\n            # Store that index value in index\n            \n            finger = f[index-2:index-1] # getting all left and right finger\n            # by indexing here we are taking all index value before \".\" which is in this case \n            # 7194c4e9-8e19-496a-9c56-56678d40b67b_4R.png here with example \"7194c4e9-8e19-496a-9c56-56678d40b67b_4R\" get stored in finger\n        \n            \n            fingers.append(finger)  # i am storing all data name but not include .png\n            \n            hand = f[index-1:index]\n         \n            hands.append(hand)# here i am storing \"L\"->left and \"R\"->right hand\n            finger_hand = f[index-2:index]  # Here i am extracting number of finger lik 4L means 4\n            both.append(finger_hand)  # adding both right & left hand plus no of finger like this ['4R']['1L']\n        Filepath_series = pd.Series(file_paths, name='filepaths')  # storing filepath in Series like \"1    ../input/fingers/train/6c9cec85-6a2f-4c6c-bf85...\"\"\n        File_series = pd.Series(fingers, name='fingers')\n        \n        Hand_series = pd.Series(hands, name='hand')  # Here I am storing R and L means right or left hand \n        \"\"\"\n        like this:\n        0        R\n        1        L\n        2        R\n        3        L\n        4        L\n        Name: hand\n        \"\"\"\n        Handpath_series = pd.Series(both, name='f&h')  # here I am storing which hand plus no of fingers like below\n        '''\n        Like this:\n        0        4R\n        1        4L\n        2        0R\n        3        4L\n        4        1L\n        Name: f&h\n        '''\n        if category == 'train':  # If data is in train directory\n            df = pd.concat([Filepath_series, File_series, Hand_series, Handpath_series], axis=1) # Here I am concatenating all features in one dataframe\n        else: \n            # data is in test directory\n            test_df = pd.concat([Filepath_series, File_series, Hand_series, Handpath_series], axis=1)  # Here I am concatenating all features in one dataframe\n    labels = df['fingers']   # this is our label/target feature/column\n    train_df, valid_df = train_test_split(df, train_size=train_split, shuffle=True, random_state=123, stratify=labels)   # Here I am spliting data into train set and validation set\n    print('train_df length:', len(train_df), 'test_df length : ', len(test_df), 'valid_df length : ', len(valid_df))\n    return train_df, test_df, valid_df","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:14.432531Z","iopub.execute_input":"2022-03-20T21:14:14.432936Z","iopub.status.idle":"2022-03-20T21:14:14.447048Z","shell.execute_reply.started":"2022-03-20T21:14:14.432901Z","shell.execute_reply":"2022-03-20T21:14:14.445664Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"src_dir = r'../input/fingers'\ntrain_split = 0.9\ntrain_df, test_df, valid_df = preprocess(src_dir, train_split)\nprint('\\n\\n',train_df.columns)\nprint('\\n\\n',train_df.head(4))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:15.577718Z","iopub.execute_input":"2022-03-20T21:14:15.578487Z","iopub.status.idle":"2022-03-20T21:14:15.685633Z","shell.execute_reply.started":"2022-03-20T21:14:15.578428Z","shell.execute_reply":"2022-03-20T21:14:15.684813Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"test_df","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:18.135175Z","iopub.execute_input":"2022-03-20T21:14:18.135847Z","iopub.status.idle":"2022-03-20T21:14:18.147776Z","shell.execute_reply.started":"2022-03-20T21:14:18.135809Z","shell.execute_reply":"2022-03-20T21:14:18.147049Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"length = len(test_df)\ntotal = []\nfor n in range(1, length+1):\n    if length % n ==0 and length/n<=80:\n        a = int(length/n)\n        total.append(a)\nprint(total)\nprint(sorted(total, reverse=True)[0])\n        \n#test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)\n#print(test_batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:20.652270Z","iopub.execute_input":"2022-03-20T21:14:20.652714Z","iopub.status.idle":"2022-03-20T21:14:20.660057Z","shell.execute_reply.started":"2022-03-20T21:14:20.652675Z","shell.execute_reply":"2022-03-20T21:14:20.659191Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"markdown","source":"The train dataset is balanced ","metadata":{}},{"cell_type":"markdown","source":"# Creating train, test and validation generators\nfirst we will build a model to be able to count the number of fingers","metadata":{}},{"cell_type":"code","source":"\"\"\"\n    :params train_df : training dataset\n    :params test_df : testing dataset\n    :params valid_df: validation dataset\n    :params target: prediction column here it is fingers\n    :return : train_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, train_steps\n    \"\"\"\n    length = len(test_df)    # 3600\n    total_len = []\n    # here i am calculating test_batch_size under 80\n    for n in range(1, length+1):\n        if length % n == 0 and length/n<=80:\n            l = int(length/n)\n            total_len.append(l)\n            # total_len will give us a list  : [80, 75, 72, 60, 50, 48, 45, 40, 36, 30, 25, 24, 20, 18, 16, 15, 12, 10, 9, 8, 6, 5, 4, 3, 2, 1]\n    test_batch_size = sorted(total_len, reverse=True)[0]   # this will give us 80  \n    test_steps = int(length/test_batch_size)  # 3600/80 = 45, 45 will the train_steps\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:24.112210Z","iopub.execute_input":"2022-03-20T21:14:24.112922Z","iopub.status.idle":"2022-03-20T21:14:24.119519Z","shell.execute_reply.started":"2022-03-20T21:14:24.112882Z","shell.execute_reply":"2022-03-20T21:14:24.118485Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"working_dir = r'./'  # working directory path\nimg_size = (128, 128)\nchannels = 3   # RGB color \nbatch_size = 40\nimg_shape = (img_size[0], img_size[1], channels)  # shape : (128, 128, 3)\n\ndef generate_data(train_df, test_df, valid_df, ycol):\n    \"\"\"\n    :params train_df : training dataset\n    :params test_df : testing dataset\n    :params valid_df: validation dataset\n    :params target: prediction column here it is fingers\n    :return : train_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, train_steps\n    \"\"\"\n    length = len(test_df)    # 3600\n    total_len = []\n    # here i am calculating test_batch_size under 80\n    for n in range(1, length+1):\n        if length % n == 0 and length/n<=80:\n            l = int(length/n)\n            total_len.append(l)\n            # total_len will give us a list  : [80, 75, 72, 60, 50, 48, 45, 40, 36, 30, 25, 24, 20, 18, 16, 15, 12, 10, 9, 8, 6, 5, 4, 3, 2, 1]\n    test_batch_size = sorted(total_len, reverse=True)[0]   # this will give us 80  \n    test_steps = int(length/test_batch_size)  # 3600/80 = 45, 45 will the train_steps\n    \n    trgen=ImageDataGenerator()\n    tvgen=ImageDataGenerator()\n    msg='for the train generator'\n    print(msg, '\\r', end='') \n    train_gen=trgen.flow_from_dataframe( train_df, x_col='filepaths', y_col=ycol, target_size=img_size, class_mode='categorical',\n                                        color_mode='rgb', shuffle=True, batch_size=batch_size)\n    msg='for the test generator'\n    print(msg, '\\r', end='') \n    test_gen=tvgen.flow_from_dataframe( test_df, x_col='filepaths', y_col=ycol, target_size=img_size, class_mode='categorical',\n                                        color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n    msg='for the validation generator'\n    print(msg, '\\r', end='')\n    valid_gen=tvgen.flow_from_dataframe( valid_df, x_col='filepaths', y_col=ycol, target_size=img_size, class_mode='categorical',\n                                        color_mode='rgb', shuffle=True, batch_size=batch_size)    \n    classes=list(train_gen.class_indices.keys())\n    class_count=len(classes)\n    train_steps=int(np.ceil(len(train_gen.labels)/batch_size))\n    labels=test_gen.labels\n    return train_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, train_steps\ntrain_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, train_steps=generate_data(train_df, test_df, valid_df,'fingers')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:14:31.170680Z","iopub.execute_input":"2022-03-20T21:14:31.171002Z","iopub.status.idle":"2022-03-20T21:14:38.881465Z","shell.execute_reply.started":"2022-03-20T21:14:31.170968Z","shell.execute_reply":"2022-03-20T21:14:38.880753Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# Display some training images samples","metadata":{}},{"cell_type":"code","source":"def show_image_samples(gen):\n    train_dict = gen.class_indices\n    \n    classes = list(train_dict.keys())\n    images, labels=next(gen)  # get a sample batch from the generator\n    plt.figure(figsize= (20, 20))\n    length = len(labels)\n    if length < 25:\n        # here i am showiing maximum of 25 images\n        r = length\n    else:\n        r = 24\n    for i in range(r):\n        # looping for 25 images\n        plt.subplot(5, 5, i+1)\n        # now normalize between 0 to 1\n        image = images[i]/255\n        plt.imshow(image)\n        index = np.argmax(labels[i])\n        class_name = classes[index]\n        plt.title(class_name, color='blue', fontsize=12)\n        plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:23:51.128058Z","iopub.execute_input":"2022-03-20T21:23:51.128417Z","iopub.status.idle":"2022-03-20T21:23:51.136285Z","shell.execute_reply.started":"2022-03-20T21:23:51.128366Z","shell.execute_reply":"2022-03-20T21:23:51.135211Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"show_image_samples(train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T21:23:52.756733Z","iopub.execute_input":"2022-03-20T21:23:52.757552Z","iopub.status.idle":"2022-03-20T21:23:55.322076Z","shell.execute_reply.started":"2022-03-20T21:23:52.757503Z","shell.execute_reply":"2022-03-20T21:23:55.321478Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"# let's create and compile the model","metadata":{}},{"cell_type":"code","source":"def make_model(img_img_size, class_count,lr=.001, trainable=True):\n    img_shape=(img_size[0], img_size[1], 3)\n    model_name='EfficientNetB3'\n    base_model=tf.keras.applications.efficientnet.EfficientNetB3(include_top=False, weights=\"imagenet\",input_shape=img_shape, pooling='max') \n    base_model.trainable=trainable\n    x=base_model.output\n    x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n    x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                    bias_regularizer=regularizers.L1(0.006) ,activation='relu')(x)\n    x=Dropout(rate=.45, seed=123)(x)             \n    output=Dense(class_count, activation='softmax')(x)\n    model=Model(inputs=base_model.input, outputs=output)\n    model.compile(Adamax(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy']) \n    return model, base_model # return the base_model so the callback can control its training state  ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T22:09:21.197569Z","iopub.execute_input":"2022-03-20T22:09:21.198282Z","iopub.status.idle":"2022-03-20T22:09:21.206070Z","shell.execute_reply.started":"2022-03-20T22:09:21.198248Z","shell.execute_reply":"2022-03-20T22:09:21.205402Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"# Instantiat the custom callback and train the model","metadata":{}},{"cell_type":"code","source":"model, base_model=make_model(img_size, class_count)\n# defaults to base_model=trainable\nepochs =80\npatience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\nstop_patience =3 # number of epochs to wait before stopping training if monitored value does not improve\nthreshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\nfactor=.5 # factor to reduce lr by\ndwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\nfreeze=False # if true free weights of  the base model\nask_epoch=5 # number of epochs to run before asking if you want to halt training\nbatches=train_steps\ncsv_path=os.path.join(working_dir,'my_csv')\ncallbacks=[ALRCallback(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,nspace=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch)]","metadata":{"execution":{"iopub.status.busy":"2022-03-20T22:35:34.126096Z","iopub.execute_input":"2022-03-20T22:35:34.126356Z","iopub.status.idle":"2022-03-20T22:35:36.766011Z","shell.execute_reply.started":"2022-03-20T22:35:34.126326Z","shell.execute_reply":"2022-03-20T22:35:36.765181Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x=train_gen, epochs=epochs,\n                   verbose=0, callbacks=callbacks, validation_data=valid_gen, validation_steps=None, shuffle=False, initial_epoch = 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T22:35:39.455548Z","iopub.execute_input":"2022-03-20T22:35:39.456234Z","iopub.status.idle":"2022-03-20T23:13:12.031860Z","shell.execute_reply.started":"2022-03-20T22:35:39.456196Z","shell.execute_reply":"2022-03-20T23:13:12.031061Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"# Plot training data for finger counting model","metadata":{}},{"cell_type":"code","source":"# define a function to plot the training data\n\ndef training_plot(train_data, start_epoch):\n    # Plot the training and validation data\n    train_acc = train_data.history['accuracy']\n    valid_acc = train_data.history['val_accuracy']\n    train_loss = train_data.history['loss']\n    valid_loss = train_data.history['val_loss']\n    Epoch_count = len(train_acc) + start_epoch\n    Epochs = []\n    for i in range(start_epoch, Epoch_count):\n        Epochs.append(i+1)\n    index_loss = np.argmin(valid_loss) # this is the epoch with the lowest validation loss\n    val_lowest = valid_loss[index_loss]\n    index_acc = np.argmax(valid_acc)\n    acc_highest = valid_acc[index_acc]\n    plt.style.use('fivethirtyeight')\n    sc_label = 'best epoch = ' + str(index_loss + 1 + start_epoch)\n    vc_label = 'best epoch = ' + str(index_acc + 1 + start_epoch)\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n    axes[0].plot(Epochs,train_loss, 'r', label='Training loss')\n    axes[0].plot(Epochs,valid_loss,'g',label='Validation loss' )\n    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n    axes[0].set_title('Training and Validation Loss')\n    axes[0].set_xlabel('Epochs')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[1].plot (Epochs,train_acc,'r',label= 'Training Accuracy')\n    axes[1].plot (Epochs,valid_acc,'g',label= 'Validation Accuracy')\n    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n    axes[1].set_title('Training and Validation Accuracy')\n    axes[1].set_xlabel('Epochs')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    plt.tight_layout\n    #plt.style.use('fivethirtyeight')\n    plt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:19:36.327030Z","iopub.execute_input":"2022-03-20T23:19:36.327293Z","iopub.status.idle":"2022-03-20T23:19:36.340635Z","shell.execute_reply.started":"2022-03-20T23:19:36.327263Z","shell.execute_reply":"2022-03-20T23:19:36.339801Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"training_plot(history, 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:19:36.869905Z","iopub.execute_input":"2022-03-20T23:19:36.870135Z","iopub.status.idle":"2022-03-20T23:19:37.340226Z","shell.execute_reply.started":"2022-03-20T23:19:36.870109Z","shell.execute_reply":"2022-03-20T23:19:37.339538Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"# Make predictions on test set create confustion matrix and classification report","metadata":{}},{"cell_type":"code","source":"def print_info(test_gen, preds, print_code, save_dir, subject):\n    class_dict = test_gen.class_indices\n    labels= test_gen.labels\n    file_names= test_gen.filenames \n    error_list=[]\n    true_class=[]\n    pred_class=[]\n    prob_list=[]\n    new_dict={}\n    error_indices=[]\n    y_pred=[]\n    for key,value in class_dict.items():\n        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n    # store new_dict as a text fine in the save_dir\n    classes=list(new_dict.values())     # list of string of class names     \n    errors=0      \n    for i, p in enumerate(preds):\n        pred_index=np.argmax(p)         \n        true_index=labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred\n            error_list.append(file_names[i])\n            true_class.append(new_dict[true_index])\n            pred_class.append(new_dict[pred_index])\n            prob_list.append(p[pred_index])\n            error_indices.append(true_index)            \n            errors=errors + 1\n        y_pred.append(pred_index) \n    tests=len(preds)\n    acc= (1-errors/tests) *100\n    msg= f'There were {errors} errors in {tests} test cases Model accuracy= {acc: 6.2f} %'\n    print_in_color(msg,(0,255,255),(55,65,80))\n    if print_code !=0:\n        if errors>0:\n            if print_code>errors:\n                r=errors\n            else:\n                r=print_code           \n            msg='{0:^30s}{1:^30s}{2:^16s}'.format('Filename', 'Predicted Class' ,  'Probability')\n            print_in_color(msg, (0,255,0),(55,65,80))\n            for i in range(r):                \n                split1=os.path.split(error_list[i])                \n                split2=os.path.split(split1[0])                \n                fname=split2[1] + '/' + split1[1]\n                msg='{0:^30s}{1:^30s}{2:4s}{3:^6.4f}'.format(fname, pred_class[i], ' ', prob_list[i])\n                print_in_color(msg, (255,255,255), (55,65,60))\n                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n        else:\n            msg='With accuracy of 100 % there are no errors to print'\n            print_in_color(msg, (0,255,0),(55,65,80))\n    if errors>0:\n        plot_bar=[]\n        plot_class=[]\n        for  key, value in new_dict.items():        \n            count=error_indices.count(key) \n            if count!=0:\n                plot_bar.append(count) # list containg how many times a class c had an error\n                plot_class.append(value)   # stores the class \n        fig=plt.figure()\n        fig.set_figheight(len(plot_class)/3)\n        fig.set_figwidth(10)\n        plt.style.use('fivethirtyeight')\n        for i in range(0, len(plot_class)):\n            c=plot_class[i]\n            x=plot_bar[i]\n            plt.barh(c, x, )\n            plt.title( ' Errors by Class on Test Set')\n    y_true= np.array(labels)        \n    y_pred=np.array(y_pred)\n    if len(classes)<= 30:\n        # create a confusion matrix \n        cm = confusion_matrix(y_true, y_pred )        \n        length=len(classes)\n        if length<8:\n            fig_width=8\n            fig_height=8\n        else:\n            fig_width= int(length * .5)\n            fig_height= int(length * .5)\n        plt.figure(figsize=(fig_width, fig_height))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n    clr = classification_report(y_true, y_pred, target_names=classes, digits= 4)\n    print(\"Classification Report:\\n----------------------\\n\", clr)\n    return acc/100","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:26:47.803045Z","iopub.execute_input":"2022-03-20T23:26:47.803358Z","iopub.status.idle":"2022-03-20T23:26:47.828482Z","shell.execute_reply.started":"2022-03-20T23:26:47.803324Z","shell.execute_reply":"2022-03-20T23:26:47.827706Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"subject='finger count'\nprint_code=0\npreds=model.predict(test_gen) \nacc=print_info( test_gen, preds, print_code, working_dir, subject ) ","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:26:48.618759Z","iopub.execute_input":"2022-03-20T23:26:48.619230Z","iopub.status.idle":"2022-03-20T23:26:53.746552Z","shell.execute_reply.started":"2022-03-20T23:26:48.619194Z","shell.execute_reply":"2022-03-20T23:26:53.745667Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"markdown","source":"### wow amazing ","metadata":{}},{"cell_type":"markdown","source":"# Let'save the finger count model and its class_dict.csv file\n","metadata":{}},{"cell_type":"code","source":"def saver(save_path, model, model_name, subject, accuracy,img_size, scalar,offset ,generator):    \n    # first save the model\n    save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n    model_save_loc=os.path.join(save_path, save_id)\n    model.save(model_save_loc)\n    print_in_color ('model was saved as ' + model_save_loc, (0,255,0),(55,65,80)) \n    # now create the class_df and convert to csv file    \n    class_dict=generator.class_indices \n    height=[]\n    width=[]\n    scale=[]\n    off=[]\n    for i in range(len(class_dict)):\n        height.append(img_size[0])\n        width.append(img_size[1])\n        scale.append(scalar) \n        off.append(offset)\n    Index_series=pd.Series(list(class_dict.values()), name='class_index')\n    Class_series=pd.Series(list(class_dict.keys()), name='class') \n    Height_series=pd.Series(height, name='height')\n    Width_series=pd.Series(width, name='width')\n    Scale_series=pd.Series(scale, name='scale by')\n    Off_series=pd.Series(off, name='Offset')\n    class_df=pd.concat([Index_series, Class_series, Height_series, Width_series, Scale_series, Off_series],axis=1)    \n    csv_name='class_dict.csv'\n    csv_save_loc=os.path.join(save_path, csv_name)\n    class_df.to_csv(csv_save_loc, index=False) \n    print_in_color ('class csv file was saved as ' + csv_save_loc, (0,255,0),(55,65,80)) \n    return model_save_loc, csv_save_loc","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:29:19.050392Z","iopub.execute_input":"2022-03-20T23:29:19.051012Z","iopub.status.idle":"2022-03-20T23:29:19.064080Z","shell.execute_reply.started":"2022-03-20T23:29:19.050972Z","shell.execute_reply":"2022-03-20T23:29:19.063334Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"model_name='EfficientNetB3'\npixel_scale=1 #EfficientNet models expect pixels in range 0 to 255 so no scaling is needed\npixel_offset=0 # No pixel offset is needed\nmodel_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, pixel_scale, pixel_offset,  train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:30:57.004775Z","iopub.execute_input":"2022-03-20T23:30:57.005476Z","iopub.status.idle":"2022-03-20T23:30:58.194168Z","shell.execute_reply.started":"2022-03-20T23:30:57.005436Z","shell.execute_reply":"2022-03-20T23:30:58.193345Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"# Now just build a model to detect which hands is in the image","metadata":{}},{"cell_type":"code","source":"K.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# recreate the generators with y_col = 'hand'\ntrain_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, batches=generate_data(train_df, test_df, valid_df,'hand') \nmodel, base_model=make_model(img_size, class_count, lr=.001, trainable=True) # recreate the model - so it starts fresh with new initialized weights\nask_epoch=5\ncallbacks=[ALRCallback(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,nspace=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch)]\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:49:50.059441Z","iopub.execute_input":"2022-03-20T23:49:50.059891Z","iopub.status.idle":"2022-03-20T23:57:42.718848Z","shell.execute_reply.started":"2022-03-20T23:49:50.059855Z","shell.execute_reply":"2022-03-20T23:57:42.716481Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"code","source":"training_plot(history,0)\nsubject='which hand'\nprint_code=0\npreds=model.predict(test_gen) \nacc=print_info( test_gen, preds, print_code, working_dir, subject ) \nmodel_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, pixel_scale, pixel_offset,  train_gen)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T23:58:40.322972Z","iopub.execute_input":"2022-03-20T23:58:40.323232Z","iopub.status.idle":"2022-03-20T23:58:49.164295Z","shell.execute_reply.started":"2022-03-20T23:58:40.323203Z","shell.execute_reply":"2022-03-20T23:58:49.163610Z"},"trusted":true},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":"# Now build model that predicts both the finger cound and the hand\n\nThe f&h column of the dataset has 12 unique classes resulting from 6 finger classes and 2 hand classes\nso our model is now trying to predict 12 classes versus 6 for the finger model and 2 for the hand model","metadata":{}},{"cell_type":"code","source":"K.clear_session() #clear the session\ntf.compat.v1.reset_default_graph()\n# recreate the generators with y_col = 'hand'\ntrain_gen, test_gen, valid_gen, classes, class_count, labels, test_steps, batches=generate_data(train_df, test_df, valid_df,'f&h') \nmodel, base_model=make_model(img_size, class_count, lr=.001, trainable=True) # recreate the model - so it starts fresh with new initialized weights\nask_epoch=5\ncallbacks=[ALRCallback(model=model,base_model= base_model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,nspace=dwell, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch)]\nhistory=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n               validation_steps=None,  shuffle=False,  initial_epoch=0)\ntraining_plot(history,0)\nsubject='f&h'\nprint_code=0\npreds=model.predict(test_gen) \nacc=print_info( test_gen, preds, print_code, working_dir, subject ) \nmodel_save_loc, csv_save_loc=saver(working_dir, model, model_name, subject, acc, img_size, pixel_scale, pixel_offset,  train_gen)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-21T00:03:09.198473Z","iopub.execute_input":"2022-03-21T00:03:09.199019Z","iopub.status.idle":"2022-03-21T00:19:30.917333Z","shell.execute_reply.started":"2022-03-21T00:03:09.198979Z","shell.execute_reply":"2022-03-21T00:19:30.916487Z"},"trusted":true},"execution_count":154,"outputs":[]}]}